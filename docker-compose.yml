version: '3.8'

services:
  genai-eng-prompt:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: genai-eng-prompt
    ports:
      - "${PORT:-3010}:3010"
    environment:
      - NODE_ENV=${NODE_ENV:-production}
      - PORT=3010
      - PROVIDER=${PROVIDER:-openai}
      # OpenAI
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4}
      # Anthropic
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - ANTHROPIC_MODEL=${ANTHROPIC_MODEL:-claude-3-opus-20240229}
      # Ollama
      - OLLAMA_URL=${OLLAMA_URL:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama2}
      # OpenRouter
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - OPENROUTER_BASE_URL=${OPENROUTER_BASE_URL:-https://openrouter.ai/api/v1}
      - OPENROUTER_MODEL=${OPENROUTER_MODEL:-openai/gpt-4}
      # Perplexity
      - PERPLEXITY_API_KEY=${PERPLEXITY_API_KEY}
      - PERPLEXITY_BASE_URL=${PERPLEXITY_BASE_URL:-https://api.perplexity.ai}
      - PERPLEXITY_MODEL=${PERPLEXITY_MODEL:-pplx-70b-online}
      # OpenWebUI
      - OPENWEBUI_API_KEY=${OPENWEBUI_API_KEY}
      - OPENWEBUI_BASE_URL=${OPENWEBUI_BASE_URL:-http://host.docker.internal:8080/api/v1}
      - OPENWEBUI_MODEL=${OPENWEBUI_MODEL:-gpt-4}
      # GenerAtiva
      - GENERATIVA_API_KEY=${GENERATIVA_API_KEY}
      - GENERATIVA_BASE_URL=${GENERATIVA_BASE_URL:-https://api.generativa.com/v1}
      - GENERATIVA_MODEL=${GENERATIVA_MODEL:-gpt-4}
    volumes:
      # Monta configurações externas (opcional)
      - ./config:/etc/rapport/genai-eng-prompt:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3010/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Serviço Ollama (opcional, descomente se quiser rodar localmente)
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: ollama
  #   ports:
  #     - "11434:11434"
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   restart: unless-stopped

# volumes:
#   ollama-data:
